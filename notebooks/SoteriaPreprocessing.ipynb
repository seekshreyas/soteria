{
 "metadata": {
  "name": "",
  "signature": "sha256:e7c9d446d7f1ef9a5b6f3f5993a89cfb0c07e3cad4054b619f47d357ba152e3d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Soteria\n",
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Team\n",
      "\n",
      "| Name | Email | Roles |\n",
      "|:-----|:------|:------|\n",
      "| Victor Starostenko | victor.starostenko@live.com | Feature Analysis, Logic Management, Coding, Project Management |\n",
      "| Evie Phan | evphan@gmail.com | Feature Analysis, Frontend Design, Wireframes and Prototyping, Logo Design|\n",
      "| Ashley DeSouza | ashley.souza@live.com | Feature Engineering, Data Cleaning and Transformation, Data Analysis |\n",
      "| Shreyas | shreyas@ischool.berkeley.edu | Coding & Development, Data Mining, Frontend develpment |\n",
      "\n",
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Many companies have suffered extremely high costs associated with losing sensitive information due to security breaches, but what is even more troubling is that these companies kept their breaches under wraps. A few years ago this was the way things were done. Nobody wanted to be exposed for having weak security or fragile infrastructure, and so organizations endured the breach, paid for the consequences, and kept quiet about the details out of embarrassment. And then a few months later the same breach would happen to someone else.\n",
      "\n",
      "By sharing as much information as possible about security breaches and what led to them, organizations will be able to more strategically and effectively fight the attackers. By uniting information, analysing it, and drawing conclusions it would be much easier to find commonalities in offending technologies or methods.\n",
      "\n",
      "####PETER GEORGE, President and Chief Executive Officer of Fidelis, says: \n",
      "\n",
      "> \"Companies should share security breach information because that is the only way we will be able to cobble together a comprehensive picture of the threats and fight back.\"\n",
      "\n",
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Soteria is a project that focuses on exploring and analysing security breach data. For our analysis we will be using a VERIS Community Database. VERIS Community Database is a dataset of over 3,000 security incidents and breaches. It is also a set of metrics designed to provide a common language for describing security incidents in a structured and repeatable manner. By analysing and extracting interesting features we aim to gain understanding of which metrics and vectors of attack are user to target specific industries. We also plan to create a visualization/report viewing interface to be able to sift through the records and relevant data.**\n",
      "\n",
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## About the Data\n",
      "\n",
      "#### Open Data Set\n",
      "[VERIS Community Database](https://github.com/vz-risk/VCDB)\n",
      "VERIS is a Vocabulary for Event Recording and Incident Sharing. VERIS is a set of metrics designed to provide a common language for describing security incidents in a structured and repeatable manner. \n",
      "VERIS Community Database a dataset of over 3,000 security incidents and breaches.\n",
      "\n",
      "#### Source\n",
      "\n",
      "[VERIS Community Database GitHub repository](http://veriscommunity.net/doku.php?id=public)\n",
      "\n",
      "[License information](https://github.com/vz-risk/VCDB/blob/master/LICENSE.txt)\n",
      "\n",
      "----------\n",
      "\n",
      "#### DataSet Description\n",
      "\n",
      "\n",
      "Looking at the data we are quickly able to extract interesting features.\n",
      "For eample, the bar graph below shows the number of breaches from 1971 to 2014:\n",
      "\n",
      "[<img src=\"https://raw.githubusercontent.com/vstarostenko/soteria/dev/charts/incidents_per_year.png\" width=\"1200\">](https://raw.githubusercontent.com/vstarostenko/soteria/dev/charts/incidents_per_year.png)\n",
      "\n",
      "*----------*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Preprocessing the dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Populating the interactive namespace from numpy and matplotlib*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab --no-import-all inline\n",
      "%matplotlib inline "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------\n",
      "*Importing relevant libraries*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 1:\n",
      "\n",
      "Some attributes in the dataset are not needed for this analysis.\n",
      "Our starting point always remains unmodified, any changes to the dataset are recorded in new CSV and JSON documents. In this step we are removing some of the attributes that we found to be not relevant to our analysis. \n",
      "\n",
      "The output is saved to a new document called: **vcdb_preprocessed.csv**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    source = \"../data/vcdb_categories_evie.csv\"\n",
      "    result = \"../data/vcdb_preprocessed.csv\"\n",
      "    removed_fields = [\"source_id\",\"summary\",\"related_incidents\",\"notes\",\"victim.locations_affected\",\"victim.notes\",\"victim.revenue.iso_currency_code\",\"victim.secondary.amount\",\"victim.secondary.victim_id\",\"asset.management\",\"asset.notes\",\"attribute.confidentiality.notes\",\"attribute.confidentiality.state\",\"attribute.integrity.notes\",\"attribute.availability.duration.unit\",\"attribute.availability.duration.value\",\"attribute.availability.notes\",\"timeline.compromise.unit\",\"timeline.compromise.value\",\"timeline.containment.unit\",\"timeline.containment.value\",\"timeline.exfiltration.unit\",\"timeline.exfiltration.value\",\"targeted\",\"impact.iso_currency_code\",\"impact.notes\",\"impact.overall_amount\",\"impact.overall_min_amount\",\"impact.overall_rating\",\"plus.analysis_status\",\"plus.analyst\",\"plus.analyst_notes\",\"plus.asset.total\",\"plus.attribute.confidentiality.credit_monitoring\",\"plus.attribute.confidentiality.credit_monitoring_years\",\"plus.attribute.confidentiality.data_abuse\",\"plus.attribute.confidentiality.data_misuse\",\"plus.attribute.confidentiality.data_subject\",\"plus.attribute.confidentiality.partner_number\",\"plus.created\",\"plus.dbir_year\",\"plus.f500\",\"plus.github\",\"plus.issue_id\",\"plus.master_id\",\"plus.modified\",\"plus.timeline.notification.day\",\"plus.timeline.notification.month\",\"plus.timeline.notification.year\",\"data_total\"]\n",
      "    removed_index = []\n",
      "    with open(source,\"rb\") as sourceh:\n",
      "        rdr= csv.reader(sourceh)\n",
      "        headers = rdr.next()\n",
      "        print len(headers)\n",
      "\n",
      "        with open(result, \"wb\") as new_csv_file:\n",
      "            wrtr = csv.writer(new_csv_file)\n",
      "            for i in range(len(headers)):\n",
      "                if headers[i] in removed_fields:\n",
      "                    removed_index.append(i)\n",
      "\n",
      "            for i in removed_index[::-1]:\n",
      "                del headers[i]\n",
      "            wrtr.writerow(headers)\n",
      "\n",
      "            for row in rdr:\n",
      "                for index in removed_index[::-1]:\n",
      "                    del row[index]\n",
      "                wrtr.writerow(row)\n",
      "\n",
      "        print len(headers)\n",
      "        \n",
      "    print \"No Errors\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "378\n",
        "328"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As can be seen from the output above, we have removed a total of 50 attributes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 2:\n",
      "\n",
      "In this step, we are converting **Industry Codes** column to corresponding industry names. \n",
      "\n",
      "We used the **Census API** to recode our data:\n",
      "http://www.census.gov/cgi-bin/sssd/naics/naicsrch?chart=2012"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    industry_index = 0\n",
      "    industry_code = {}\n",
      "    naics_file = \"../data/2-digit_2012_Codes.csv\"\n",
      "    preprocess_file = \"../data/vcdb_preprocessed.csv\"\n",
      "    preprocessed_file = \"../data/vcdb_industry_processed.csv\"\n",
      "    \n",
      "    with open(naics_file, \"rb\") as csv_file:\n",
      "        reader = csv.reader(csv_file)\n",
      "        header = reader.next()\n",
      "        \n",
      "        for row in reader:\n",
      "            industry_code[row[1]] = row[2]\n",
      "    \n",
      "    with open(preprocess_file, \"rb\") as csv_file:\n",
      "        reader = csv.reader(csv_file)\n",
      "        header = reader.next()\n",
      "        \n",
      "        for i in range(len(header)):\n",
      "            if header[i] == \"victim.industry\":\n",
      "                industry_index = i\n",
      "                break\n",
      "            \n",
      "        with open(preprocessed_file, \"wb\") as new_csv_file:\n",
      "            writer = csv.writer(new_csv_file)\n",
      "            writer.writerow(header)\n",
      "            for row in reader:\n",
      "                if row[industry_index] in industry_code:\n",
      "                    row[industry_index] = industry_code[row[industry_index]]\n",
      "                else:\n",
      "                    row[industry_index] = None\n",
      "                writer.writerow(row)\n",
      "                \n",
      "    print \"No Errors\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To visualize what the code above did we compare the two dataframes.\n",
      "These are the first 5 rows from the original, coded column:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_industry_coded = pd.read_csv(\"../data/vcdb_preprocessed.csv\")\n",
      "df_industry_recoded = pd.read_csv(\"../data/vcdb_industry_processed.csv\")\n",
      "\n",
      "df_industry_coded[['victim.industry']].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>victim.industry</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  51919</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 561990</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 221118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 813410</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 524114</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 1 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "   victim.industry\n",
        "0            51919\n",
        "1           561990\n",
        "2           221118\n",
        "3           813410\n",
        "4           524114\n",
        "\n",
        "[5 rows x 1 columns]"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And these are the same 5 rows from the recoded dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_industry_recoded[['victim.industry']].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>victim.industry</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>                All Other Information Services</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>                    All Other Support Services</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>              Other Electric Power Generation </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>               Civic and Social Organizations </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> Direct Health and Medical Insurance Carriers </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 1 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "                                 victim.industry\n",
        "0                 All Other Information Services\n",
        "1                     All Other Support Services\n",
        "2               Other Electric Power Generation \n",
        "3                Civic and Social Organizations \n",
        "4  Direct Health and Medical Insurance Carriers \n",
        "\n",
        "[5 rows x 1 columns]"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the generated CSV file can be used to plot a bar chart representing breaches by industry:\n",
      "\n",
      "[<img src=\"https://raw.githubusercontent.com/vstarostenko/soteria/dev/charts/incident_by_industry.png\" width=\"1400\">](https://raw.githubusercontent.com/vstarostenko/soteria/dev/charts/incident_by_industry.png)\n",
      "\n",
      "----------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 3:\n",
      "\n",
      "One of the challenges with the data was the way breach discovery timeline was represented. This timeline was represented by two attributes: **timeline.discovery.unit** and **timeline.discovery.value**. \n",
      "\n",
      "These attributes were not normalized and spread out across multiple values, which represented seconds, minutes, hours, days, weeks, months, and years. Taking the median of these values (days), we recoded each value in terms of days. The new column describes how many days it took for the breach to be discovered.\n",
      "\n",
      "The new attribute is called: **timeline.discovery.day_count**.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    unit = \"\"\n",
      "    value = 0\n",
      "    with open(\"../data/vcdb_industry_processed.csv\", \"rb\") as csv_file:\n",
      "        rdr = csv.reader(csv_file)\n",
      "        header = rdr.next()\n",
      "\n",
      "        with open(\"../data/vcdb_date_processed.csv\", \"wb\") as new_csv_file:\n",
      "            wrtr = csv.writer(new_csv_file)\n",
      "            for i in range(len(header)):\n",
      "                if header[i] == \"timeline.discovery.unit\":\n",
      "                    unit = i\n",
      "                elif header[i] == \"timeline.discovery.value\":\n",
      "                    value = i\n",
      "                else:\n",
      "                    pass\n",
      "            header[value] = \"timeline.discovery.day_count\"    \n",
      "            del header[unit]\n",
      "            wrtr.writerow(header)\n",
      "\n",
      "            for row in rdr:\n",
      "                if row[value] == \"\":\n",
      "                    if row[unit] == \"Seconds\":\n",
      "                        row[value] = 0.0007\n",
      "                    elif row[unit] == \"Minutes\":\n",
      "                        row[value] = 0.021\n",
      "                    elif row[unit] == \"Hours\":\n",
      "                        row[value] = 0.5\n",
      "                    elif row[unit] == \"Weeks\":\n",
      "                        row[value] = 14\n",
      "                    elif row[unit] == \"Days\":\n",
      "                        row[value] = 15\n",
      "                    elif row[unit] == \"Months\":\n",
      "                        row[value] = 182\n",
      "                    elif row[unit] == \"Years\":\n",
      "                        row[value] = 365\n",
      "                    elif row[unit] == \"Unknown\":\n",
      "                        row[value] = None\n",
      "                else:\n",
      "                    if row[unit] == \"Seconds\":\n",
      "                        row[value] = int(row[value])/(24.0*60.0*60.0)\n",
      "                    elif row[unit] == \"Minutes\":\n",
      "                        row[value] = int(row[value])/(24.0*60.0)\n",
      "                    elif row[unit] == \"Hours\":\n",
      "                        row[value] = int(row[value])/24.0\n",
      "                    elif row[unit] == \"Weeks\":\n",
      "                        row[value] = int(row[value])*7\n",
      "                    elif row[unit] == \"Days\":\n",
      "                        row[value] = int(row[value])\n",
      "                    elif row[unit] == \"Months\":\n",
      "                        row[value] = int(row[value])*30\n",
      "                    elif row[unit] == \"Years\":\n",
      "                        row[value] = int(row[value])*365\n",
      "                del row[unit]\n",
      "                wrtr.writerow(row)\n",
      "    print \"No Errors\"\n",
      "            \n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the original set of columns:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_timeline_discovery = pd.read_csv(\"../data/vcdb_industry_processed.csv\")\n",
      "df_timeline_discovery[['timeline.discovery.unit', 'timeline.discovery.value']].head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>timeline.discovery.unit</th>\n",
        "      <th>timeline.discovery.value</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>     NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>     NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>     NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  Months</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  Months</td>\n",
        "      <td>  5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> Unknown</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>     NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>  Months</td>\n",
        "      <td>  5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td> Unknown</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> Unknown</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>10 rows \u00d7 2 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "  timeline.discovery.unit  timeline.discovery.value\n",
        "0                     NaN                       NaN\n",
        "1                     NaN                       NaN\n",
        "2                     NaN                       NaN\n",
        "3                  Months                         2\n",
        "4                  Months                         5\n",
        "5                 Unknown                       NaN\n",
        "6                     NaN                       NaN\n",
        "7                  Months                         5\n",
        "8                 Unknown                       NaN\n",
        "9                 Unknown                       NaN\n",
        "\n",
        "[10 rows x 2 columns]"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And this is the recoded column:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_timeline_discovery = pd.read_csv(\"../data/vcdb_date_processed.csv\")\n",
      "df_timeline_discovery[['timeline.discovery.day_count']].head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>timeline.discovery.day_count</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  60</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 150</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td> 150</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>10 rows \u00d7 1 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "   timeline.discovery.day_count\n",
        "0                           NaN\n",
        "1                           NaN\n",
        "2                           NaN\n",
        "3                            60\n",
        "4                           150\n",
        "5                           NaN\n",
        "6                           NaN\n",
        "7                           150\n",
        "8                           NaN\n",
        "9                           NaN\n",
        "\n",
        "[10 rows x 1 columns]"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 4:\n",
      "\n",
      "Some of the attributes had many missing values and were of no significance to our project. These values are removed in the code below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    source = \"../data/vcdb_date_processed.csv\"\n",
      "    result = \"../data/vcdb_columns_removed_processed.csv\"\n",
      "    remove_fields = [\"asset.accessibility\",\"asset.cloud\",\"asset.hosting\",\"asset.ownership\", \n",
      "                      \"asset.assets.Kiosk.Term\", \"asset.assets.Media\"]\n",
      "    remove_index = []\n",
      "    \n",
      "    with open(source,\"rb\") as csv_file:\n",
      "        rdr= csv.reader(csv_file)\n",
      "        headers = rdr.next()\n",
      "        \n",
      "        with open(result, \"wb\") as new_csv_file:\n",
      "            wrtr = csv.writer(new_csv_file)\n",
      "            for i in range(len(headers)):\n",
      "                if headers[i] in remove_fields:\n",
      "                    remove_index.append(i)\n",
      "            \n",
      "            for i in remove_index[::-1]:    \n",
      "                del headers[i]\n",
      "            wrtr.writerow(headers)\n",
      "            \n",
      "            for row in rdr:\n",
      "                for index in remove_index[::-1]:\n",
      "                    del row[index]\n",
      "                wrtr.writerow(row)\n",
      "    print \"No Errors\"\n",
      "            \n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 5:\n",
      "\n",
      "Another attribute that had to be recoded was **victim.employee_count**. Original values in this attributes were of the following type: \n",
      "\n",
      "*Over 100000, \n",
      "Unknown, \n",
      "1 to 10, \n",
      "1001 to 10000*\n",
      "\n",
      "We wanted to convert these to a common numerical notation. The code below achieves this:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    preprocess_file = \"../data/vcdb_columns_removed_processed.csv\"\n",
      "    preprocessed_file = \"../data/vcdb_empcount_processed.csv\"\n",
      "\n",
      "    with open(preprocess_file, \"rb\") as csv_file:\n",
      "            reader = csv.reader(csv_file)\n",
      "            header = reader.next()\n",
      "\n",
      "            for i in range(len(header)):\n",
      "                if header[i] == \"victim.employee_count\":\n",
      "                    employee_count = i\n",
      "\n",
      "            with open(preprocessed_file, \"wb\") as new_csv_file:\n",
      "                writer = csv.writer(new_csv_file)\n",
      "                writer.writerow(header)\n",
      "                for row in reader:\n",
      "                    if row[employee_count] == \"Unknown\":\n",
      "                        row[employee_count] = \"NaN\"\n",
      "                        #print \"Unknown found\"\n",
      "                    elif row[employee_count].split()[0] == \"Over\":\n",
      "                        #print \"found\"\n",
      "                        row[employee_count] = float(row[employee_count].split()[1])+float(row[employee_count].split()[1])/2\n",
      "                    elif len(row[employee_count].split()) == 3 and row[employee_count].split()[1] == \"to\":\n",
      "                        row[employee_count] = (float(row[employee_count].split()[0]+row[employee_count].split()[2])/2)\n",
      "                    writer.writerow(row)\n",
      "     \n",
      "    print \"No Errors\"\n",
      "            \n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e\n",
      "\n",
      "# victor"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 6:\n",
      "\n",
      "48 of the attributes had \"...\" in their names. In order for MapReduce to function properly (beyond this notebook) we had to replace the \"...\" with \"_\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    source = \"../data/vcdb_empcount_processed.csv\"\n",
      "    result = \"../data/vcdb_fully_processed.csv\"\n",
      "    df_elipses = pd.read_csv(\"../data/vcdb_empcount_processed.csv\")\n",
      "\n",
      "    new_headers = []\n",
      "\n",
      "    with open(source,\"rb\") as csv_file:\n",
      "            rdr= csv.reader(csv_file)\n",
      "            headers = rdr.next()\n",
      "            for header in headers:\n",
      "                if '...' in header:\n",
      "                    header = header.replace(\"...\",\"_\")\n",
      "                new_headers.append(header)\n",
      "                \n",
      "    df_elipses.columns = new_headers\n",
      "    df_elipses.to_csv(\"../data/vcdb_fully_processed.csv\")\n",
      "    \n",
      "    print \"No Errors\"\n",
      "            \n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 7:\n",
      "\n",
      "In this step we are taking care of missing values. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(\"../data/vcdb_fully_processed.csv\")\n",
      "df.to_csv(\"../data/vcdb_fully_processed.csv\", na_rep='NaN',index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 8:\n",
      "\n",
      "In this step we are converting our final preprocessed CSV to JSON format. The JSON file will be used for data mining and MapReduce."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_json = pd.read_csv(\"../data/vcdb_fully_processed.csv\")\n",
      "\n",
      "df_json.to_json(\"../data/vcdb_fully_processed.json\", orient='records')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The dataset is now ready to be proess by an EC2 cluster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Similar Breaches\n",
      "\n",
      "When we looked at __breaches__ on their own with respect to the features provided, the features looked _sparse_, they were booleans and we felt like on their own, they were not enough to deduct a conclusion from.\n",
      "\n",
      "So, we decided to look at __pairwise combination of breaches__ on those same features and figure out how __similar__ they were. These are key attributes that a security professional would like to look at, as given these parameters of the breach, were there other breaches that were similar to it, and hence such an analysis would add more context to the breach being scrutinized at the moment.\n",
      "\n",
      "As we had `3,084` records, doing a __pairwise combination of those breaches__ would result in `4,753,986` possible combinations, we had to develop our script for distributed computing. \n",
      "\n",
      "Hence we developed a __map-reduce__ script using python __MRJob__ library, to calculate the similarity of breaches"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The script below is produced for evaluation purposes. To run the script please run it from the command line using the following commands"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Script Dependencies\n",
      "\n",
      "```\n",
      "- Numpy\n",
      "- MRJob \n",
      "- Scikit-learn\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Attack_similarity.py script\n",
      "\n",
      "```python\n",
      "\n",
      "#! /usr/bin/env python\n",
      "from __future__ import division\n",
      "from mrjob.job import MRJob\n",
      "from itertools import combinations\n",
      "# from sklearn.metrics import jaccard_similarity_score\n",
      "import numpy as np\n",
      "import sys\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class AttackSimilarity(MRJob):\n",
      "    # INPUT_PROTOCOL = JSONValueProtocol\n",
      "\n",
      "\n",
      "    def extract_incident(self, _, line):\n",
      "        record = line.split(',')\n",
      "\n",
      "        # print record\n",
      "        if record[0] != 'incident_id':\n",
      "            feature = record[1:]\n",
      "            incident = record[0]\n",
      "\n",
      "\n",
      "            yield incident, list(feature)\n",
      "\n",
      "\n",
      "    def combine_incident(self, incident, feature):\n",
      "        allfeatures = list(feature)\n",
      "\n",
      "        yield incident, list(allfeatures[0])\n",
      "\n",
      "\n",
      "    def distribute_incident(self, incd, incdfeat):\n",
      "        yield \"all\" , [incd, list(incdfeat)]\n",
      "\n",
      "\n",
      "    def similar_incident(self, _, allincidents):\n",
      "        for (inc_a, feat_a), (inc_b, feat_b) in combinations(list(allincidents), r=2):\n",
      "\n",
      "            feat_a_array = np.array(feat_a, dtype='int')\n",
      "            feat_b_array = np.array(feat_b, dtype='int')\n",
      "\n",
      "\n",
      "            # similarity = jaccard_similarity_score(feat_a_array, feat_b_array)\n",
      "            feat_a_mag = np.sqrt(np.dot(feat_a_array, feat_a_array))\n",
      "            feat_b_mag = np.sqrt(np.dot(feat_b_array, feat_a_array))\n",
      "\n",
      "            similarity = float(np.dot(feat_a_array, feat_b_array))/ (feat_a_mag * feat_b_mag)\n",
      "\n",
      "            sys.stderr.write(\"Similarity: ({0},{1})\\n\".format([inc_a, inc_b],similarity))\n",
      "\n",
      "            if similarity > 0.90 :\n",
      "                yield [inc_a, inc_b], similarity\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    def steps(self):\n",
      "        \"\"\"\n",
      "        MapReduce Steps:\n",
      "\n",
      "        extract_incident    :   <_, line>  =>  <incident, feature>\n",
      "        combine_incident    :   <incident, [feature]> => <incident, allfeatures>\n",
      "        map_incident        :   <incident, [incedentfeatures] => <\"all\", [[incident, features]]\n",
      "        reduce_incident     :   <_, allincidents> => <[incident_pairs], similarity>\n",
      "        \"\"\"\n",
      "\n",
      "        return [\n",
      "            self.mr(mapper=self.extract_incident, reducer=self.combine_incident),\n",
      "            self.mr(mapper=self.distribute_incident, reducer=self.similar_incident)\n",
      "        ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    AttackSimilarity.run()\n",
      "\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### To run the script locally\n",
      "\n",
      "- test the script by streaming first 10 lines of records\n",
      "```\n",
      "$head -n 10 data/vcdb_mrjob2.csv | python attack_similiarity.py\n",
      "```\n",
      "\n",
      "- to run the script fully on local machine\n",
      "```\n",
      "$ python attack_similarity.py data/vcdb_mrjob2.csv\n",
      "```\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sample Local output for first 10 rows of records\n",
      "\n",
      "Command\n",
      "\n",
      "```bash\n",
      "$ head -n 10 data/vcdb_mrjob2.csv | python attack_similiarity.py\n",
      "```\n",
      "\n",
      "Output\n",
      "```bash\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534\n",
      "reading from STDIN\n",
      "writing to /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534/step-0-mapper-sorted\n",
      "> sort /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534/step-0-mapper_part-00000\n",
      "writing to /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534/step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534/step-1-mapper-sorted\n",
      "> sort /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534/step-1-mapper_part-00000\n",
      "writing to /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534/step-1-reducer_part-00000\n",
      "attack_similiarity.py:47: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  similarity = float(np.dot(feat_a_array, feat_b_array))/ (feat_a_mag * feat_b_mag)\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534/step-1-reducer_part-00000 -> /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534/output/part-00000\n",
      "Streaming final output from /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534/output\n",
      "[\"002599D4-A872-433B-9980-BD9F257B283F\", \"0096EF99-D9CB-4869-9F3D-F4E0D84F419B\"]\t0.94868329805051377\n",
      "[\"002599D4-A872-433B-9980-BD9F257B283F\", \"00EB741C-0DFD-453E-9AC2-B00E512897DA\"]\t0.94868329805051377\n",
      "[\"0096EF99-D9CB-4869-9F3D-F4E0D84F419B\", \"00EB741C-0DFD-453E-9AC2-B00E512897DA\"]\t0.99999999999999978\n",
      "[\"00DCB3AD-AF4C-4AC1-8A6E-682DB697C727\", \"00EB741C-0DFD-453E-9AC2-B00E512897DA\"]\t0.94280904158206325\n",
      "removing tmp directory /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185609.078534\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### To run the script on an EMR cluster\n",
      "\n",
      "Do the following steps:\n",
      "- Create an mrjob.conf file for configuration settings\n",
      "- copy your permissions file (`<permissionfilename>.pem`) from amazon\n",
      "- change the access settings of your permission file through `chmod` command:\n",
      "    - `$sudo chmod 600 <permissionfilename>.pem`\n",
      "    \n",
      " "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Sample MRJOB.conf file\n",
      "\n",
      "```\n",
      "runners:\n",
      "  emr:\n",
      "    aws_access_key_id: <AMAZON KEY>\n",
      "    aws_secret_access_key: <AMAZON SECRET KEY>\n",
      "    ec2_core_instance_type: m1.large\n",
      "    ec2_key_pair: <KEY-PAIR NAME>\n",
      "    ec2_key_pair_file: <ABSOLUTE PATH TO KEY-FILE : permissions.pem >\n",
      "    num_ec2_core_instances: 5\n",
      "    pool_wait_minutes: 2\n",
      "    pool_emr_job_flows: true\n",
      "    ssh_tunnel_is_open: true\n",
      "    ssh_tunnel_to_job_tracker: true\n",
      "\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Fire the script as:\n",
      "\n",
      "- check the settings first with:\n",
      "\n",
      "```bash\n",
      "$ head -n 10 data/vcdb_mrjob2.csv | python attack_similiarity.py -c mrjob.conf -r emr\n",
      "```\n",
      "\n",
      "- full map reduce job\n",
      "\n",
      "```bash\n",
      "$ time python attack_similiarity.py data/vcdb_mrjob2.csv -r emr -c mrjob.conf\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sample EMR Run output for first 10 records\n",
      "\n",
      "Command\n",
      "```bash\n",
      "$ head -n 10 data/vcdb_mrjob2.csv | python attack_similiarity.py -c mrjob.conf -r emr\n",
      "```\n",
      "\n",
      "Output\n",
      "```bash\n",
      "using existing scratch bucket mrjob-51b9493c1a467671\n",
      "using s3://mrjob-51b9493c1a467671/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185623.814405\n",
      "writing master bootstrap script to /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185623.814405/b.py\n",
      "reading from STDIN\n",
      "Copying non-input files into s3://mrjob-51b9493c1a467671/tmp/attack_similiarity.shreyas.20140506.185623.814405/files/\n",
      "Attempting to find an available job flow...\n",
      "When AMI version is set to 'latest', job flow pooling can result in the job being added to a pool using an older AMI version\n",
      "Adding our job to existing job flow j-19L00U5JOP27C\n",
      "Job launched 30.4s ago, status RUNNING: Running step (attack_similiarity.shreyas.20140506.185623.814405: Step 1 of 2)\n",
      "Opening ssh tunnel to Hadoop job tracker\n",
      "Connect to job tracker at: http://1.0.0.127.in-addr.arpa:40760/jobtracker.jsp\n",
      "Job launched 62.1s ago, status RUNNING: Running step (attack_similiarity.shreyas.20140506.185623.814405: Step 1 of 2)\n",
      "Unable to load progress from job tracker\n",
      "Job launched 92.6s ago, status RUNNING: Running step (attack_similiarity.shreyas.20140506.185623.814405: Step 1 of 2)\n",
      "Job launched 123.0s ago, status RUNNING: Running step (attack_similiarity.shreyas.20140506.185623.814405: Step 1 of 2)\n",
      "Job launched 153.4s ago, status RUNNING: Running step (attack_similiarity.shreyas.20140506.185623.814405: Step 2 of 2)\n",
      "Job launched 183.9s ago, status RUNNING: Running step (attack_similiarity.shreyas.20140506.185623.814405: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 189.0s (not counting time spent waiting for the EC2 instances)\n",
      "Fetching counters from SSH...\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 187909\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 13995\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 1531\n",
      "    FILE_BYTES_WRITTEN: 1050825\n",
      "    HDFS_BYTES_READ: 4380\n",
      "    HDFS_BYTES_WRITTEN: 13995\n",
      "    S3_BYTES_READ: 187909\n",
      "  Job Counters :\n",
      "    Launched map tasks: 30\n",
      "    Launched reduce tasks: 8\n",
      "    Rack-local map tasks: 30\n",
      "    SLOTS_MILLIS_MAPS: 264350\n",
      "    SLOTS_MILLIS_REDUCES: 162928\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 64670\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 15814\n",
      "    Map input records: 10\n",
      "    Map output bytes: 14013\n",
      "    Map output materialized bytes: 5293\n",
      "    Map output records: 9\n",
      "    Physical memory (bytes) snapshot: 13905297408\n",
      "    Reduce input groups: 9\n",
      "    Reduce input records: 9\n",
      "    Reduce output records: 9\n",
      "    Reduce shuffle bytes: 5293\n",
      "    SPLIT_RAW_BYTES: 4380\n",
      "    Spilled Records: 18\n",
      "    Total committed heap usage (bytes): 14167834624\n",
      "    Virtual memory (bytes) snapshot: 50262282240\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 46251\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 404\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 1397\n",
      "    FILE_BYTES_WRITTEN: 1188752\n",
      "    HDFS_BYTES_READ: 51921\n",
      "    S3_BYTES_WRITTEN: 404\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 31\n",
      "    Launched map tasks: 35\n",
      "    Launched reduce tasks: 8\n",
      "    Rack-local map tasks: 2\n",
      "    SLOTS_MILLIS_MAPS: 273029\n",
      "    SLOTS_MILLIS_REDUCES: 145161\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 30780\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 13995\n",
      "    Map input records: 9\n",
      "    Map output bytes: 14094\n",
      "    Map output materialized bytes: 5978\n",
      "    Map output records: 9\n",
      "    Physical memory (bytes) snapshot: 16541917184\n",
      "    Reduce input groups: 1\n",
      "    Reduce input records: 9\n",
      "    Reduce output records: 4\n",
      "    Reduce shuffle bytes: 5978\n",
      "    SPLIT_RAW_BYTES: 5670\n",
      "    Spilled Records: 18\n",
      "    Total committed heap usage (bytes): 16876306432\n",
      "    Virtual memory (bytes) snapshot: 57137254400\n",
      "Streaming final output from s3://mrjob-51b9493c1a467671/tmp/attack_similiarity.shreyas.20140506.185623.814405/output/\n",
      "[\"00EB741C-0DFD-453E-9AC2-B00E512897DA\", \"002599D4-A872-433B-9980-BD9F257B283F\"]\t0.94868329805051377\n",
      "[\"00EB741C-0DFD-453E-9AC2-B00E512897DA\", \"0096EF99-D9CB-4869-9F3D-F4E0D84F419B\"]\t0.99999999999999978\n",
      "[\"002599D4-A872-433B-9980-BD9F257B283F\", \"0096EF99-D9CB-4869-9F3D-F4E0D84F419B\"]\t0.94868329805051377\n",
      "[\"00DCB3AD-AF4C-4AC1-8A6E-682DB697C727\", \"0096EF99-D9CB-4869-9F3D-F4E0D84F419B\"]\t0.94280904158206325\n",
      "removing tmp directory /var/folders/5p/jqdjg7z572d5d40t2pfc2nh80000gn/T/attack_similiarity.shreyas.20140506.185623.814405\n",
      "Removing all files in s3://mrjob-51b9493c1a467671/tmp/attack_similiarity.shreyas.20140506.185623.814405/\n",
      "\n",
      "```\n",
      "\n",
      "For the entire dataset, following is the time statistics\n",
      "\n",
      "```\n",
      "real\t36m51.968s\n",
      "user\t0m2.935s\n",
      "    sys\t0m0.753s\n",
      "```\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "### Similarity Conclusions\n",
      "\n",
      "- measures of similarity changed the output.\n",
      "    - `Jaccard Similarity` earlier, which gave us `24,442` for similarity condition of exact match 1.0\n",
      "    - `Cosine Similarity` gave us `102670` records for similarity condition of > 0.9\n",
      "\n",
      "- Although the feature matrices were sparse, we got a lot of high similarity values. \n",
      "    - Looking at the results, our intuition is to believe the following about security data\n",
      "        - the veris vocabulary might still need more features as maybe the features aren't granular enough to understand the very different natures of each breach\n",
      "        - in security space, the new hacks coming in every year are too few. And similar attacks are used with different targets.\n",
      "\n",
      "But to dig deeper into these intuitions, we did step 2\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 2:\n",
      "\n",
      "In this step we generate a side-by-side comparison of various attributes that we believe are determinating factors of the after-effects of a breach. This is done for each victim combination that is returned from the Map-Reduce step. \n",
      "The results are stored in a csv file (Location: ../data/vcdb_similarity_compariosn.csv)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv\n",
      "import re\n",
      "\n",
      "def main():\n",
      "    markdown_file = \"../similar_attacks.md\"\n",
      "    preprocessed_file = \"../data/vcdb_fully_processed.csv\"\n",
      "    output_file = \"../data/vcdb_similarity_comparison.csv\"\n",
      "    relevant_attributes = [\"incident_id\", \"industry.categories\", \"victim.victim_id\", \"timeline.incident.month\", \"timeline.incident.year\", \"timeline.discovery.day_count\", \"attribute.confidentiality.data_total\", \"actor.internal\", \"actor.external\"]\n",
      "    incident_id = {}\n",
      "    industry_ids = []\n",
      "    relevant_attributes_index = []\n",
      "    row_attributes = []\n",
      "    new_header = []\n",
      "    new_row = []\n",
      "    victim_num = 1\n",
      "    regex = r'\\w{8}\\-\\w{4}\\-\\w{4}\\-\\w{4}\\-\\w{12}'\n",
      "    \n",
      "    with open(markdown_file, \"rb\") as md_file:\n",
      "        for i in md_file:\n",
      "            searchObj = re.findall(regex, i, flags = 0)\n",
      "            if searchObj:\n",
      "                industry_ids.append((searchObj[0], searchObj[1]))\n",
      "            else:\n",
      "                pass\n",
      "            \n",
      "    with open(output_file, \"wb\") as new_csv_file:\n",
      "        wrtr = csv.writer(new_csv_file)        \n",
      "        with open(preprocessed_file, \"rb\") as csv_file:\n",
      "            reader = csv.reader(csv_file)\n",
      "            headers = reader.next()\n",
      "            \n",
      "            for i in range(len(headers)):\n",
      "                if headers[i] in relevant_attributes:\n",
      "                    relevant_attributes_index.append(i)\n",
      "                else:\n",
      "                    pass\n",
      "            \n",
      "            for i in range(len(relevant_attributes_index)):\n",
      "                new_header.append(\"victim\"+ str(victim_num) + \"_\" + headers[relevant_attributes_index[i]])\n",
      "                victim_num += 1\n",
      "                new_header.append(\"victim\"+ str(victim_num) + \"_\" + headers[relevant_attributes_index[i]])\n",
      "                victim_num -= 1\n",
      "            wrtr.writerow(new_header)\n",
      "                     \n",
      "            for row in reader:                \n",
      "                row_attributes= []\n",
      "                for i in range(1,len(relevant_attributes_index)):\n",
      "                    row_attributes.append(row[relevant_attributes_index[i]])\n",
      "                incident_id[row[relevant_attributes_index[0]]] = row_attributes\n",
      "                \n",
      "            for key in range(len(industry_ids)):\n",
      "                new_row = []\n",
      "                if industry_ids[key][0] in incident_id.keys() and industry_ids[key][1] in incident_id.keys():                    \n",
      "                    new_row.append(industry_ids[key][0])\n",
      "                    for vic_one in incident_id[industry_ids[key][0]]:\n",
      "                        new_row.append(vic_one)\n",
      "                    idx_vic_2 = 1\n",
      "                    new_row.insert(idx_vic_2, industry_ids[key][1])\n",
      "                    for vic_two in incident_id[industry_ids[key][1]]:\n",
      "                        idx_vic_2 += 2\n",
      "                        new_row.insert(idx_vic_2, vic_two)\n",
      "                    wrtr.writerow(new_row)                    \n",
      "                else:\n",
      "                    pass\n",
      "                \n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 3:\n",
      "\n",
      "In this step we perform further analysis on the victims of security breaches. \n",
      "Here we generate another csv file that provides statistical information about each security breach such as the number of times that particular breach occured. \n",
      "\n",
      "The input file is the output file that is generated from the previous step, i.e. the file at location \"../data/vcdb_similarity_comparison.csv\"\n",
      "\n",
      "The resulting output file is stored at location \"../data/vcdb_similarity_comparison_2.csv\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import read_csv\n",
      "from urllib import urlopen\n",
      "import csv\n",
      "page = urlopen(\"../data/vcdb_similarity_comparison.csv\")\n",
      "df = read_csv(page)\n",
      "\n",
      "grouped_object = df.groupby(\"victim1_incident_id\")\n",
      "grouped_object[\"victim1_victim.victim_id\"].describe().to_csv(\"../data/vcdb_similarity_comparison_2.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}