{
 "metadata": {
  "name": "",
  "signature": "sha256:e7c9d446d7f1ef9a5b6f3f5993a89cfb0c07e3cad4054b619f47d357ba152e3d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Soteria\n",
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Team\n",
      "\n",
      "| Name | Email | Roles |\n",
      "|:-----|:------|:------|\n",
      "| Victor Starostenko | victor.starostenko@live.com | Feature Analysis, Logic Management, Coding, Project Management |\n",
      "| Evie Phan | evphan@gmail.com | Feature Analysis, Frontend Design, Wireframes and Prototyping, Logo Design|\n",
      "| Ashley DeSouza | ashley.souza@live.com | Feature Engineering, Data Cleaning and Transformation, Data Analysis |\n",
      "| Shreyas | shreyas@ischool.berkeley.edu | Coding & Development, Data Mining, Frontend develpment |\n",
      "\n",
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Many companies have suffered extremely high costs associated with losing sensitive information due to security breaches, but what is even more troubling is that these companies kept their breaches under wraps. A few years ago this was the way things were done. Nobody wanted to be exposed for having weak security or fragile infrastructure, and so organizations endured the breach, paid for the consequences, and kept quiet about the details out of embarrassment. And then a few months later the same breach would happen to someone else.\n",
      "\n",
      "By sharing as much information as possible about security breaches and what led to them, organizations will be able to more strategically and effectively fight the attackers. By uniting information, analysing it, and drawing conclusions it would be much easier to find commonalities in offending technologies or methods.\n",
      "\n",
      "####PETER GEORGE, President and Chief Executive Officer of Fidelis, says: \n",
      "\n",
      "> \"Companies should share security breach information because that is the only way we will be able to cobble together a comprehensive picture of the threats and fight back.\"\n",
      "\n",
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Soteria is a project that focuses on exploring and analysing security breach data. For our analysis we will be using a VERIS Community Database. VERIS Community Database is a dataset of over 3,000 security incidents and breaches. It is also a set of metrics designed to provide a common language for describing security incidents in a structured and repeatable manner. By analysing and extracting interesting features we aim to gain understanding of which metrics and vectors of attack are user to target specific industries. We also plan to create a visualization/report viewing interface to be able to sift through the records and relevant data.**\n",
      "\n",
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## About the Data\n",
      "\n",
      "#### Open Data Set\n",
      "[VERIS Community Database](https://github.com/vz-risk/VCDB)\n",
      "VERIS is a Vocabulary for Event Recording and Incident Sharing. VERIS is a set of metrics designed to provide a common language for describing security incidents in a structured and repeatable manner. \n",
      "VERIS Community Database a dataset of over 3,000 security incidents and breaches.\n",
      "\n",
      "#### Source\n",
      "\n",
      "[VERIS Community Database GitHub repository](http://veriscommunity.net/doku.php?id=public)\n",
      "\n",
      "[License information](https://github.com/vz-risk/VCDB/blob/master/LICENSE.txt)\n",
      "\n",
      "----------\n",
      "\n",
      "#### DataSet Description\n",
      "\n",
      "\n",
      "Looking at the data we are quickly able to extract interesting features.\n",
      "For eample, the bar graph below shows the number of breaches from 1971 to 2014:\n",
      "\n",
      "[<img src=\"https://raw.githubusercontent.com/vstarostenko/soteria/dev/charts/incidents_per_year.png\" width=\"1200\">](https://raw.githubusercontent.com/vstarostenko/soteria/dev/charts/incidents_per_year.png)\n",
      "\n",
      "*----------*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Preprocessing the dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Populating the interactive namespace from numpy and matplotlib*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab --no-import-all inline\n",
      "%matplotlib inline "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------\n",
      "*Importing relevant libraries*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 1:\n",
      "\n",
      "Some attributes in the dataset are not needed for this analysis.\n",
      "Our starting point always remains unmodified, any changes to the dataset are recorded in new CSV and JSON documents. In this step we are removing some of the attributes that we found to be not relevant to our analysis. \n",
      "\n",
      "The output is saved to a new document called: **vcdb_preprocessed.csv**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    source = \"../data/vcdb_categories_evie.csv\"\n",
      "    result = \"../data/vcdb_preprocessed.csv\"\n",
      "    removed_fields = [\"source_id\",\"summary\",\"related_incidents\",\"notes\",\"victim.locations_affected\",\"victim.notes\",\"victim.revenue.iso_currency_code\",\"victim.secondary.amount\",\"victim.secondary.victim_id\",\"asset.management\",\"asset.notes\",\"attribute.confidentiality.notes\",\"attribute.confidentiality.state\",\"attribute.integrity.notes\",\"attribute.availability.duration.unit\",\"attribute.availability.duration.value\",\"attribute.availability.notes\",\"timeline.compromise.unit\",\"timeline.compromise.value\",\"timeline.containment.unit\",\"timeline.containment.value\",\"timeline.exfiltration.unit\",\"timeline.exfiltration.value\",\"targeted\",\"impact.iso_currency_code\",\"impact.notes\",\"impact.overall_amount\",\"impact.overall_min_amount\",\"impact.overall_rating\",\"plus.analysis_status\",\"plus.analyst\",\"plus.analyst_notes\",\"plus.asset.total\",\"plus.attribute.confidentiality.credit_monitoring\",\"plus.attribute.confidentiality.credit_monitoring_years\",\"plus.attribute.confidentiality.data_abuse\",\"plus.attribute.confidentiality.data_misuse\",\"plus.attribute.confidentiality.data_subject\",\"plus.attribute.confidentiality.partner_number\",\"plus.created\",\"plus.dbir_year\",\"plus.f500\",\"plus.github\",\"plus.issue_id\",\"plus.master_id\",\"plus.modified\",\"plus.timeline.notification.day\",\"plus.timeline.notification.month\",\"plus.timeline.notification.year\",\"data_total\"]\n",
      "    removed_index = []\n",
      "    with open(source,\"rb\") as sourceh:\n",
      "        rdr= csv.reader(sourceh)\n",
      "        headers = rdr.next()\n",
      "        print len(headers)\n",
      "\n",
      "        with open(result, \"wb\") as new_csv_file:\n",
      "            wrtr = csv.writer(new_csv_file)\n",
      "            for i in range(len(headers)):\n",
      "                if headers[i] in removed_fields:\n",
      "                    removed_index.append(i)\n",
      "\n",
      "            for i in removed_index[::-1]:\n",
      "                del headers[i]\n",
      "            wrtr.writerow(headers)\n",
      "\n",
      "            for row in rdr:\n",
      "                for index in removed_index[::-1]:\n",
      "                    del row[index]\n",
      "                wrtr.writerow(row)\n",
      "\n",
      "        print len(headers)\n",
      "        \n",
      "    print \"No Errors\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "378\n",
        "328"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As can be seen from the output above, we have removed a total of 50 attributes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 2:\n",
      "\n",
      "In this step, we are converting **Industry Codes** column to corresponding industry names. \n",
      "\n",
      "We used the **Census API** to recode our data:\n",
      "http://www.census.gov/cgi-bin/sssd/naics/naicsrch?chart=2012"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    industry_index = 0\n",
      "    industry_code = {}\n",
      "    naics_file = \"../data/2-digit_2012_Codes.csv\"\n",
      "    preprocess_file = \"../data/vcdb_preprocessed.csv\"\n",
      "    preprocessed_file = \"../data/vcdb_industry_processed.csv\"\n",
      "    \n",
      "    with open(naics_file, \"rb\") as csv_file:\n",
      "        reader = csv.reader(csv_file)\n",
      "        header = reader.next()\n",
      "        \n",
      "        for row in reader:\n",
      "            industry_code[row[1]] = row[2]\n",
      "    \n",
      "    with open(preprocess_file, \"rb\") as csv_file:\n",
      "        reader = csv.reader(csv_file)\n",
      "        header = reader.next()\n",
      "        \n",
      "        for i in range(len(header)):\n",
      "            if header[i] == \"victim.industry\":\n",
      "                industry_index = i\n",
      "                break\n",
      "            \n",
      "        with open(preprocessed_file, \"wb\") as new_csv_file:\n",
      "            writer = csv.writer(new_csv_file)\n",
      "            writer.writerow(header)\n",
      "            for row in reader:\n",
      "                if row[industry_index] in industry_code:\n",
      "                    row[industry_index] = industry_code[row[industry_index]]\n",
      "                else:\n",
      "                    row[industry_index] = None\n",
      "                writer.writerow(row)\n",
      "                \n",
      "    print \"No Errors\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To visualize what the code above did we compare the two dataframes.\n",
      "These are the first 5 rows from the original, coded column:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_industry_coded = pd.read_csv(\"../data/vcdb_preprocessed.csv\")\n",
      "df_industry_recoded = pd.read_csv(\"../data/vcdb_industry_processed.csv\")\n",
      "\n",
      "df_industry_coded[['victim.industry']].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>victim.industry</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  51919</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 561990</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 221118</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 813410</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 524114</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 1 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "   victim.industry\n",
        "0            51919\n",
        "1           561990\n",
        "2           221118\n",
        "3           813410\n",
        "4           524114\n",
        "\n",
        "[5 rows x 1 columns]"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And these are the same 5 rows from the recoded dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_industry_recoded[['victim.industry']].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>victim.industry</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>                All Other Information Services</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>                    All Other Support Services</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>              Other Electric Power Generation </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>               Civic and Social Organizations </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> Direct Health and Medical Insurance Carriers </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 1 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "                                 victim.industry\n",
        "0                 All Other Information Services\n",
        "1                     All Other Support Services\n",
        "2               Other Electric Power Generation \n",
        "3                Civic and Social Organizations \n",
        "4  Direct Health and Medical Insurance Carriers \n",
        "\n",
        "[5 rows x 1 columns]"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the generated CSV file can be used to plot a bar chart representing breaches by industry:\n",
      "\n",
      "[<img src=\"https://raw.githubusercontent.com/vstarostenko/soteria/dev/charts/incident_by_industry.png\" width=\"1400\">](https://raw.githubusercontent.com/vstarostenko/soteria/dev/charts/incident_by_industry.png)\n",
      "\n",
      "----------\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 3:\n",
      "\n",
      "One of the challenges with the data was the way breach discovery timeline was represented. This timeline was represented by two attributes: **timeline.discovery.unit** and **timeline.discovery.value**. \n",
      "\n",
      "These attributes were not normalized and spread out across multiple values, which represented seconds, minutes, hours, days, weeks, months, and years. Taking the median of these values (days), we recoded each value in terms of days. The new column describes how many days it took for the breach to be discovered.\n",
      "\n",
      "The new attribute is called: **timeline.discovery.day_count**.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    unit = \"\"\n",
      "    value = 0\n",
      "    with open(\"../data/vcdb_industry_processed.csv\", \"rb\") as csv_file:\n",
      "        rdr = csv.reader(csv_file)\n",
      "        header = rdr.next()\n",
      "\n",
      "        with open(\"../data/vcdb_date_processed.csv\", \"wb\") as new_csv_file:\n",
      "            wrtr = csv.writer(new_csv_file)\n",
      "            for i in range(len(header)):\n",
      "                if header[i] == \"timeline.discovery.unit\":\n",
      "                    unit = i\n",
      "                elif header[i] == \"timeline.discovery.value\":\n",
      "                    value = i\n",
      "                else:\n",
      "                    pass\n",
      "            header[value] = \"timeline.discovery.day_count\"    \n",
      "            del header[unit]\n",
      "            wrtr.writerow(header)\n",
      "\n",
      "            for row in rdr:\n",
      "                if row[value] == \"\":\n",
      "                    if row[unit] == \"Seconds\":\n",
      "                        row[value] = 0.0007\n",
      "                    elif row[unit] == \"Minutes\":\n",
      "                        row[value] = 0.021\n",
      "                    elif row[unit] == \"Hours\":\n",
      "                        row[value] = 0.5\n",
      "                    elif row[unit] == \"Weeks\":\n",
      "                        row[value] = 14\n",
      "                    elif row[unit] == \"Days\":\n",
      "                        row[value] = 15\n",
      "                    elif row[unit] == \"Months\":\n",
      "                        row[value] = 182\n",
      "                    elif row[unit] == \"Years\":\n",
      "                        row[value] = 365\n",
      "                    elif row[unit] == \"Unknown\":\n",
      "                        row[value] = None\n",
      "                else:\n",
      "                    if row[unit] == \"Seconds\":\n",
      "                        row[value] = int(row[value])/(24.0*60.0*60.0)\n",
      "                    elif row[unit] == \"Minutes\":\n",
      "                        row[value] = int(row[value])/(24.0*60.0)\n",
      "                    elif row[unit] == \"Hours\":\n",
      "                        row[value] = int(row[value])/24.0\n",
      "                    elif row[unit] == \"Weeks\":\n",
      "                        row[value] = int(row[value])*7\n",
      "                    elif row[unit] == \"Days\":\n",
      "                        row[value] = int(row[value])\n",
      "                    elif row[unit] == \"Months\":\n",
      "                        row[value] = int(row[value])*30\n",
      "                    elif row[unit] == \"Years\":\n",
      "                        row[value] = int(row[value])*365\n",
      "                del row[unit]\n",
      "                wrtr.writerow(row)\n",
      "    print \"No Errors\"\n",
      "            \n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the original set of columns:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_timeline_discovery = pd.read_csv(\"../data/vcdb_industry_processed.csv\")\n",
      "df_timeline_discovery[['timeline.discovery.unit', 'timeline.discovery.value']].head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>timeline.discovery.unit</th>\n",
        "      <th>timeline.discovery.value</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>     NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>     NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>     NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  Months</td>\n",
        "      <td>  2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  Months</td>\n",
        "      <td>  5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> Unknown</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td>     NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td>  Months</td>\n",
        "      <td>  5</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td> Unknown</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> Unknown</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>10 rows \u00d7 2 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "  timeline.discovery.unit  timeline.discovery.value\n",
        "0                     NaN                       NaN\n",
        "1                     NaN                       NaN\n",
        "2                     NaN                       NaN\n",
        "3                  Months                         2\n",
        "4                  Months                         5\n",
        "5                 Unknown                       NaN\n",
        "6                     NaN                       NaN\n",
        "7                  Months                         5\n",
        "8                 Unknown                       NaN\n",
        "9                 Unknown                       NaN\n",
        "\n",
        "[10 rows x 2 columns]"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And this is the recoded column:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_timeline_discovery = pd.read_csv(\"../data/vcdb_date_processed.csv\")\n",
      "df_timeline_discovery[['timeline.discovery.day_count']].head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>timeline.discovery.day_count</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  60</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 150</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7</th>\n",
        "      <td> 150</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9</th>\n",
        "      <td> NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>10 rows \u00d7 1 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "   timeline.discovery.day_count\n",
        "0                           NaN\n",
        "1                           NaN\n",
        "2                           NaN\n",
        "3                            60\n",
        "4                           150\n",
        "5                           NaN\n",
        "6                           NaN\n",
        "7                           150\n",
        "8                           NaN\n",
        "9                           NaN\n",
        "\n",
        "[10 rows x 1 columns]"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 4:\n",
      "\n",
      "Some of the attributes had many missing values and were of no significance to our project. These values are removed in the code below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    source = \"../data/vcdb_date_processed.csv\"\n",
      "    result = \"../data/vcdb_columns_removed_processed.csv\"\n",
      "    remove_fields = [\"asset.accessibility\",\"asset.cloud\",\"asset.hosting\",\"asset.ownership\", \n",
      "                      \"asset.assets.Kiosk.Term\", \"asset.assets.Media\"]\n",
      "    remove_index = []\n",
      "    \n",
      "    with open(source,\"rb\") as csv_file:\n",
      "        rdr= csv.reader(csv_file)\n",
      "        headers = rdr.next()\n",
      "        \n",
      "        with open(result, \"wb\") as new_csv_file:\n",
      "            wrtr = csv.writer(new_csv_file)\n",
      "            for i in range(len(headers)):\n",
      "                if headers[i] in remove_fields:\n",
      "                    remove_index.append(i)\n",
      "            \n",
      "            for i in remove_index[::-1]:    \n",
      "                del headers[i]\n",
      "            wrtr.writerow(headers)\n",
      "            \n",
      "            for row in rdr:\n",
      "                for index in remove_index[::-1]:\n",
      "                    del row[index]\n",
      "                wrtr.writerow(row)\n",
      "    print \"No Errors\"\n",
      "            \n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 5:\n",
      "\n",
      "Another attribute that had to be recoded was **victim.employee_count**. Original values in this attributes were of the following type: \n",
      "\n",
      "*Over 100000, \n",
      "Unknown, \n",
      "1 to 10, \n",
      "1001 to 10000*\n",
      "\n",
      "We wanted to convert these to a common numerical notation. The code below achieves this:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    preprocess_file = \"../data/vcdb_columns_removed_processed.csv\"\n",
      "    preprocessed_file = \"../data/vcdb_empcount_processed.csv\"\n",
      "\n",
      "    with open(preprocess_file, \"rb\") as csv_file:\n",
      "            reader = csv.reader(csv_file)\n",
      "            header = reader.next()\n",
      "\n",
      "            for i in range(len(header)):\n",
      "                if header[i] == \"victim.employee_count\":\n",
      "                    employee_count = i\n",
      "\n",
      "            with open(preprocessed_file, \"wb\") as new_csv_file:\n",
      "                writer = csv.writer(new_csv_file)\n",
      "                writer.writerow(header)\n",
      "                for row in reader:\n",
      "                    if row[employee_count] == \"Unknown\":\n",
      "                        row[employee_count] = \"NaN\"\n",
      "                        #print \"Unknown found\"\n",
      "                    elif row[employee_count].split()[0] == \"Over\":\n",
      "                        #print \"found\"\n",
      "                        row[employee_count] = float(row[employee_count].split()[1])+float(row[employee_count].split()[1])/2\n",
      "                    elif len(row[employee_count].split()) == 3 and row[employee_count].split()[1] == \"to\":\n",
      "                        row[employee_count] = (float(row[employee_count].split()[0]+row[employee_count].split()[2])/2)\n",
      "                    writer.writerow(row)\n",
      "     \n",
      "    print \"No Errors\"\n",
      "            \n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e\n",
      "\n",
      "# victor"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 6:\n",
      "\n",
      "48 of the attributes had \"...\" in their names. In order for MapReduce to function properly (beyond this notebook) we had to replace the \"...\" with \"_\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main():\n",
      "    source = \"../data/vcdb_empcount_processed.csv\"\n",
      "    result = \"../data/vcdb_fully_processed.csv\"\n",
      "    df_elipses = pd.read_csv(\"../data/vcdb_empcount_processed.csv\")\n",
      "\n",
      "    new_headers = []\n",
      "\n",
      "    with open(source,\"rb\") as csv_file:\n",
      "            rdr= csv.reader(csv_file)\n",
      "            headers = rdr.next()\n",
      "            for header in headers:\n",
      "                if '...' in header:\n",
      "                    header = header.replace(\"...\",\"_\")\n",
      "                new_headers.append(header)\n",
      "                \n",
      "    df_elipses.columns = new_headers\n",
      "    df_elipses.to_csv(\"../data/vcdb_fully_processed.csv\")\n",
      "    \n",
      "    print \"No Errors\"\n",
      "            \n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "No Errors\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 7:\n",
      "\n",
      "In this step we are taking care of missing values. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(\"../data/vcdb_fully_processed.csv\")\n",
      "df.to_csv(\"../data/vcdb_fully_processed.csv\", na_rep='NaN',index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Step 8:\n",
      "\n",
      "In this step we are converting our final preprocessed CSV to JSON format. The JSON file will be used for data mining and MapReduce."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_json = pd.read_csv(\"../data/vcdb_fully_processed.csv\")\n",
      "\n",
      "df_json.to_json(\"../data/vcdb_fully_processed.json\", orient='records')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The dataset is now ready to be proess by an EC2 cluster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Data Exploration\n",
      "###Step 1:\n",
      "\n",
      "Map-Reduce to be filled in by Shreyas"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 2:\n",
      "\n",
      "In this step we generate a side-by-side comparison of various attributes that we believe are determinating factors of the after-effects of a breach. This is done for each victim combination that is returned from the Map-Reduce step. \n",
      "The results are stored in a csv file (Location: ../data/vcdb_similarity_compariosn.csv)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import csv\n",
      "import re\n",
      "\n",
      "def main():\n",
      "    markdown_file = \"../similar_attacks.md\"\n",
      "    preprocessed_file = \"../data/vcdb_fully_processed.csv\"\n",
      "    output_file = \"../data/vcdb_similarity_comparison.csv\"\n",
      "    relevant_attributes = [\"incident_id\", \"industry.categories\", \"victim.victim_id\", \"timeline.incident.month\", \"timeline.incident.year\", \"timeline.discovery.day_count\", \"attribute.confidentiality.data_total\", \"actor.internal\", \"actor.external\"]\n",
      "    incident_id = {}\n",
      "    industry_ids = []\n",
      "    relevant_attributes_index = []\n",
      "    row_attributes = []\n",
      "    new_header = []\n",
      "    new_row = []\n",
      "    victim_num = 1\n",
      "    regex = r'\\w{8}\\-\\w{4}\\-\\w{4}\\-\\w{4}\\-\\w{12}'\n",
      "    \n",
      "    with open(markdown_file, \"rb\") as md_file:\n",
      "        for i in md_file:\n",
      "            searchObj = re.findall(regex, i, flags = 0)\n",
      "            if searchObj:\n",
      "                industry_ids.append((searchObj[0], searchObj[1]))\n",
      "            else:\n",
      "                pass\n",
      "            \n",
      "    with open(output_file, \"wb\") as new_csv_file:\n",
      "        wrtr = csv.writer(new_csv_file)        \n",
      "        with open(preprocessed_file, \"rb\") as csv_file:\n",
      "            reader = csv.reader(csv_file)\n",
      "            headers = reader.next()\n",
      "            \n",
      "            for i in range(len(headers)):\n",
      "                if headers[i] in relevant_attributes:\n",
      "                    relevant_attributes_index.append(i)\n",
      "                else:\n",
      "                    pass\n",
      "            \n",
      "            for i in range(len(relevant_attributes_index)):\n",
      "                new_header.append(\"victim\"+ str(victim_num) + \"_\" + headers[relevant_attributes_index[i]])\n",
      "                victim_num += 1\n",
      "                new_header.append(\"victim\"+ str(victim_num) + \"_\" + headers[relevant_attributes_index[i]])\n",
      "                victim_num -= 1\n",
      "            wrtr.writerow(new_header)\n",
      "                     \n",
      "            for row in reader:                \n",
      "                row_attributes= []\n",
      "                for i in range(1,len(relevant_attributes_index)):\n",
      "                    row_attributes.append(row[relevant_attributes_index[i]])\n",
      "                incident_id[row[relevant_attributes_index[0]]] = row_attributes\n",
      "                \n",
      "            for key in range(len(industry_ids)):\n",
      "                new_row = []\n",
      "                if industry_ids[key][0] in incident_id.keys() and industry_ids[key][1] in incident_id.keys():                    \n",
      "                    new_row.append(industry_ids[key][0])\n",
      "                    for vic_one in incident_id[industry_ids[key][0]]:\n",
      "                        new_row.append(vic_one)\n",
      "                    idx_vic_2 = 1\n",
      "                    new_row.insert(idx_vic_2, industry_ids[key][1])\n",
      "                    for vic_two in incident_id[industry_ids[key][1]]:\n",
      "                        idx_vic_2 += 2\n",
      "                        new_row.insert(idx_vic_2, vic_two)\n",
      "                    wrtr.writerow(new_row)                    \n",
      "                else:\n",
      "                    pass\n",
      "                \n",
      "if __name__ == \"__main__\":\n",
      "    try:\n",
      "        main()\n",
      "    except Exception as e:\n",
      "        print 'Something went wrong ', e\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 3:\n",
      "\n",
      "In this step we perform further analysis on the victims of security breaches. \n",
      "Here we generate another csv file that provides statistical information about each security breach such as the number of times that particular breach occured. \n",
      "\n",
      "The input file is the output file that is generated from the previous step, i.e. the file at location \"../data/vcdb_similarity_comparison.csv\"\n",
      "\n",
      "The resulting output file is stored at location \"../data/vcdb_similarity_comparison_2.csv\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import read_csv\n",
      "from urllib import urlopen\n",
      "import csv\n",
      "page = urlopen(\"../data/vcdb_similarity_comparison.csv\")\n",
      "df = read_csv(page)\n",
      "\n",
      "grouped_object = df.groupby(\"victim1_incident_id\")\n",
      "grouped_object[\"victim1_victim.victim_id\"].describe().to_csv(\"../data/vcdb_similarity_comparison_2.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}